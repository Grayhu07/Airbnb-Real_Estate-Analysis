---
title: "Part I: Scraping Real Estate Data"
author: "Wenrui Cai"
date: 'December, 2017'
output:
  html_document:
    fig_height: 3
    fig_width: 5
  pdf_document: default
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments()
```
<!-- Don't edit the material above this line -->

##Introduction
Here I am scraping data from Real Estate and comparing it with Airbnb Data!

This project will compare listing I obtained from Airbnb open Data Source with the scraped Real Estate Data from the website casaclick.it. I will focus on three cities from Italy (Florence, Turin and Venice) and use Selector Gadget (a chrome extension) on the website to scrape the Real Estate Listing by neighborhoods. 

The end goal for this project is to answer to the question: "If I were to buy a house in a certain neighborhood within the three cities, which one will have a higher return on investement (ROI)? (In other words, which one, if rented out with Airbnb, which neighborhood will give me back more quickly the amount of money I spent on buying the house?)". 

We will scrape the first page of each neighborhoods for each city, then we will compare the neiborhoods with the Areas given in the Airbnb Data and answer to the question.

```{r, echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
library(DataComputing)
library(devtools) 
```

### Get the links
Next, we get the links from each neighborhood. Each city have a base link and the neighborhoods within the cities differ by the last 4 digits in the link, with an increment of 10.

#### Florence links
```{r, warning=FALSE}
nums = seq(1000, 1170, by=10)
URLF = "http://www.casaclick.it/vendita/residenziale/Toscana/Firenze/Firenze/A1/pag1/A1.html?idZona[]="
#initialize list of links, there will be 18 elements in this list. It stores the links
linksF <- vector("list", 18)    

#loop through nums to get the 18 links I need
for(i in 1:range(nums)){
    #assign temporary number
    temp_num = nums[i]
    #combine number with URL to get the complete link
    link = paste(URLF, temp_num, sep='')
    #append to the list
    linksF[i] = link
}
linksF[[1]] #Here is the first link
```

Then we will do the same with the other two cities.

#### Venice links
```{r, warning=FALSE}
nums = seq(2000, 2130, by=10)
URLV = "http://www.casaclick.it/vendita/residenziale/Veneto/Venezia/Venezia/A1/pag1/A1.html?idZona[]="
#There will be 14 elements in this list 
linksV <- vector("list", 14)  

for(i in 1:range(nums)){
    temp_num = nums[i]
    link = paste(URLV, temp_num, sep='')
    linksV[i] = link
    }
```

#### Turin links

```{r, warning=FALSE}
nums = seq(1200, 1350, by=10)
URLT = "http://www.casaclick.it/vendita/residenziale/Piemonte/Torino/Torino/A1/pag1/A1.html?idZona[]="
#There will be 16 elements in this list 
linksT <- vector("list", 16)    

for(i in 1:range(nums)){
    temp_num = nums[i]
    link = paste(URLT, temp_num, sep='')
    linksT[i] = link
    }
``` 

## Scraping
Now, it's time to scrape the data using Selector Gadget App! First I obtained all the Xpaths using the chrome extension for 1.neighborhoods 2. prices, then I will loop through the links I have stored in the respective vectors using the Xpaths. 

### Getting the XPaths
```{r}
neighXPath <- 'br+ span'
priceXPath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "bottom", " " ))]//strong[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]'
```

####Getting Florence Data

Here I am scraping the Florence data. Each link has 15 listings (or less) listings / observations.

```{r, message=FALSE, warning=FALSE}
library(rvest)
library(xml2) 
#Will initialize the two variables for storing the data scraped
priceF <- 0
neighF <- 0

#I am using a for loop to go through the links using the scraping method learned in class with the Xpaths
for(i in 1:18){
  price<- linksF[[i]] %>%
    read_html() %>%
    html_nodes(xpath = priceXPath) %>%
    html_text()
  priceF <- append(priceF, price)
  
  neigh <- linksF[[i]] %>%
   read_html() %>%
    html_nodes("br+ span") %>%
    html_text()
  neighF <- append(neighF, neigh)
}

#With the gsub function we clean the data that comes with a bunch of \n\n\n\t\t etc 
neighF <- gsub("\n", "", neighF)
neighF <- gsub("\t", "", neighF)
priceF <- gsub("\n", "", priceF)
priceF <- gsub("\t", "", priceF)
priceF <- gsub("     ", "", priceF)
priceF <- gsub("€", "", priceF)
priceF <- as.numeric(priceF)

#Put the two variables neigh price in a dataframe and rename the variables for future use
tableF <- data.frame(neighF, priceF) %>%
  rename('neigh' = 'neighF') %>%
  rename('price' = 'priceF') 
#Delete the first row of zeros because of initialization
tableF <- tableF[-1, ]
knitr::kable(head(tableF, 5))
```

Now I have the scraped data of Florence in a nice table!

####Getting the Venice Data

```{r, message=FALSE, warning=FALSE}
priceV <- 0 
neighV <- 0

for(i in 1:14){
  price<- linksV[[i]] %>%
    read_html() %>%
    html_nodes(xpath = priceXPath) %>%
    html_text()
  priceV <- append(priceV, price)
  
  neigh <- linksV[[i]] %>%
   read_html() %>%
    html_nodes("br+ span") %>%
    html_text()
  neighV <- append(neighV, neigh)
}

neighV <- gsub("\n", "", neighV)
neighV <- gsub("\t", "", neighV)
priceV <- gsub("\n", "", priceV)
priceV <- gsub("\t", "", priceV)
priceV <- gsub("     ", "", priceV)
priceV <- gsub("€", "", priceV)
priceV <- as.numeric(priceV)

tableV <- data.frame(neighV, priceV) %>%
  rename('neigh' = 'neighV') %>%
  rename('price' = 'priceV')
tableV <- tableV[-1, ]

```

#### Getting the Turin Data

```{r, message=FALSE, warning=FALSE}
priceT <- 0
neighT <- 0

for(i in 1:16){
  price<- linksT[[i]] %>%
    read_html() %>%
    html_nodes(xpath = priceXPath) %>%
    html_text()
  priceT <- append(priceT, price)
  
  neigh <- linksT[[i]] %>%
   read_html() %>%
    html_nodes("br+ span") %>%
    html_text()
  neighT <- append(neighT, neigh)
}

neighT <- gsub("\n", "", neighT)
neighT <- gsub("\t", "", neighT)
priceT <- gsub("\n", "", priceT)
priceT <- gsub("\t", "", priceT)
priceT <- gsub("     ", "", priceT)
priceT <- gsub("€", "", priceT)
priceT <- as.numeric(priceT)

tableT <- data.frame(neighT, priceT) %>%
  rename('neigh' = 'neighT') %>%
  rename('price' = 'priceT')
tableT <- tableT[-1, ]
```

Finally I combine the tables of the three cities through the variables renamed neigh and price and I export the table to a csv file for easier use in the next Analysis part.

```{r}
table<- rbind(tableF, tableT, tableV)

write.csv(table, "table.csv")
```

This project is only half way done...!! 

[I will link here the second part of the project where I analyze the data](Airbnb+Real_Estate.html)
