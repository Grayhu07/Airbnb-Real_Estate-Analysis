---
title: "Scraping Real Estate Data"
author: "Wenrui Cai"
date: ''
output:
  html_document:
    fig_height: 3
    fig_width: 5
  pdf_document: default
---
<!-- Don't edit in between this line and the one below -->
```{r include=FALSE}
# Don't delete this chunk if you are using the DataComputing package
library(DataComputing)
```
*Source file* 
```{r, results='asis', echo=FALSE}
includeSourceDocuments()
```
<!-- Don't edit the material above this line -->

##Introduction
Here I am scraping data from Real Estate and comparing it with Airbnb Data!

This project will compare listing I obtained from Airbnb open Data Source with the scraped Real Estate Data from the website casaclick.it. I will focus on three cities from Italy (Florence, Turin and Venice) and use Selector Gadget (a chrome extension) on the website to scrape the Real Estate Listing by neighborhoods. The end goal for this project is to answer to the question: "If I were to buy a house in a certain neighborhood within the three cities, which one will profit me the most through Airbnb? (In other words, which one, if rented out with Airbnb, which neighborhood will more likely give me back the amount of money I spent on buying the house?)". 
We will scrape the first page of each neighborhoods for each city, then we will compare the neiborhoods with the Areas given in the Airbnb Data and answer to the question.

### Load the libraries
```{r, echo=TRUE, eval=FALSE, warning=FALSE, message=FALSE}
library(DataComputing)
library(rvest)
#Needed for the XPaths
library(devtools) 
library(rvest) 
```

### Get the links
Next, we get the links from each neighborhood. We notice that each city have a base link, then the neighborhoods within the cities differ by the last 4 digits in the link, with an increment of 10. Because of this pattern, we can create a vector that stores the different links. We will first obtain Florence's links. 

```{r, warning=FALSE}
#Florence
nums = seq(1000, 1170, by=10)
URLF = "http://www.casaclick.it/vendita/residenziale/Toscana/Firenze/Firenze/A1/pag1/A1.html?idZona[]="
#initialize list of links, there will be 18 elements in this list 
linksF <- vector("list", 18)    

#loop through nums to get the 18 links I need
for(i in 1:range(nums)){
    #assign temporary number
    temp_num = nums[i]
    #combine number with URL to get the complete link
    link = paste(URLF, temp_num, sep='')
    #append to the list
    linksF[i] = link
}
linksF[[1]] #For testing purposes, here is the first link
```

Then we will do the same with the other two cities.
```{r}
#Venice
nums = seq(2000, 2130, by=10)
URLV = "http://www.casaclick.it/vendita/residenziale/Veneto/Venezia/Venezia/A1/pag1/A1.html?idZona[]="
#There will be 14 elements in this list 
linksV <- vector("list", 14)  

for(i in 1:range(nums)){
    temp_num = nums[i]
    link = paste(URLV, temp_num, sep='')
    linksV[i] = link
    }

#Turin
nums = seq(1200, 1350, by=10)
URLT = "http://www.casaclick.it/vendita/residenziale/Piemonte/Torino/Torino/A1/pag1/A1.html?idZona[]="
#There will be 16 elements in this list 
linksT <- vector("list", 16)    

for(i in 1:range(nums)){
    temp_num = nums[i]
    link = paste(URLT, temp_num, sep='')
    linksT[i] = link
    }
``` 

## Scraping
Now, it's time to scrape the data using Selector Gadget App! First I obtained all the Xpaths using the chrome extension for 1.each listing, 2.neighborhoods 3. prices.
We will first initialize the variables for the respective prices and neighborhoods, then I will loop through the links I have stored in the respective vectors using the Xpaths. 

### Getting the XPaths
```{r}
XPath <- '//strong[(((count(preceding-sibling::*) + 1) = 1) and parent::*)] | //span'
neighXPath <- '//*[(@id = "box_ricerca")]//span'
priceXPath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "bottom", " " ))]//strong[(((count(preceding-sibling::*) + 1) = 1) and parent::*)]'
```

###Getting Florence Data
First, I will scrape the Florence data. Each link we scrape will have 15 listings (or less) listings, therefore each neighborhood will have at most 15 observations. 
```{r}
library(rvest)
library(xml2) 
#Will initialize the two variables for storing the data scraped
priceF <- 0
neighF <- 0

#I am using a for loop to go through the links using the scraping method learned in class with the Xpaths
for(i in 1:18){
  price<- linksF[[i]] %>%
    read_html() %>%
    html_nodes(xpath = priceXPath) %>%
    html_text()
  priceF <- append(priceF, price)
  
  neigh <- linksF[[i]] %>%
   read_html() %>%
    html_nodes("br+ span") %>%
    html_text()
  neighF <- append(neighF, neigh)
}

#With the gsub function we clean the data that comes with a bunch of \n\n\n\t\t etc 
neighF <- gsub("\n", "", neighF)
neighF <- gsub("\t", "", neighF)
priceF <- gsub("\n", "", priceF)
priceF <- gsub("\t", "", priceF)
priceF <- gsub("     ", "", priceF)
priceF <- gsub("€", "", priceF)
#Finally convert price to a numeric variable
priceF <- as.numeric(priceF)

#Put the two variables neigh price in a dataframe and rename the variables for future use
tableF <- data.frame(neighF, priceF) %>%
  rename('neigh' = 'neighF') %>%
  rename('price' = 'priceF') 
tableF <- tableF[-1, ]
head(tableF)
```
Now I have the scraped data of Florence in a nice table!

###Getting the Venice Data
```{r}
priceV <- 0 
neighV <- 0

for(i in 1:14){
  price<- linksV[[i]] %>%
    read_html() %>%
    html_nodes(xpath = priceXPath) %>%
    html_text()
  priceV <- append(priceV, price)
  
  neigh <- linksV[[i]] %>%
   read_html() %>%
    html_nodes("br+ span") %>%
    html_text()
  neighV <- append(neighV, neigh)
}

neighV <- gsub("\n", "", neighV)
neighV <- gsub("\t", "", neighV)
priceV <- gsub("\n", "", priceV)
priceV <- gsub("\t", "", priceV)
priceV <- gsub("     ", "", priceV)
priceV <- gsub("€", "", priceV)
priceV <- as.numeric(priceV)

tableV <- data.frame(neighV, priceV) %>%
  rename('neigh' = 'neighV') %>%
  rename('price' = 'priceV')
tableV <- tableV[-1, ]

```

### Getting the Turin Data
```{r}
priceT <- 0
neighT <- 0

for(i in 1:16){
  price<- linksT[[i]] %>%
    read_html() %>%
    html_nodes(xpath = priceXPath) %>%
    html_text()
  priceT <- append(priceT, price)
  
  neigh <- linksT[[i]] %>%
   read_html() %>%
    html_nodes("br+ span") %>%
    html_text()
  neighT <- append(neighT, neigh)
}

neighT <- gsub("\n", "", neighT)
neighT <- gsub("\t", "", neighT)
priceT <- gsub("\n", "", priceT)
priceT <- gsub("\t", "", priceT)
priceT <- gsub("     ", "", priceT)
priceT <- gsub("€", "", priceT)
priceT <- as.numeric(priceT)

tableT <- data.frame(neighT, priceT) %>%
  rename('neigh' = 'neighT') %>%
  rename('price' = 'priceT')
tableT <- tableT[-1, ]
```

```{r}
#Finally I rbind the tables of the three cities through the variables renamed neigh and price
table<- rbind(tableF, tableT, tableV)
#And I export the table to a csv file for easier use in the next file
#write.csv(table, "table.csv")
```

This project is only half way done...!! 
[I will link here the second part of the project where I analyze the data](Airbnb_+_Real_Estate.html)
